{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import manifold\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"last\"\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "If we talk about **classification** problems, the most common metrics used are:\n",
    "- Accuracy  \n",
    "- Precision (P) \n",
    "- Recall (R)\n",
    "- F1 score (F1)\n",
    "- Area under the ROC (Receiver Operating Characteristic) curve or simply  AUC (AUC)  \n",
    "- Log loss  \n",
    "- Precision at k (P@k) \n",
    "- Average precision at k (AP@k)  \n",
    "- Mean average precision at k (MAP@k)  \n",
    "\n",
    "When it comes to **regression**, the most commonly used evaluation metrics are: \n",
    "- Mean absolute error (MAE) \n",
    "- Mean squared error (MSE)  \n",
    "- Root mean squared error (RMSE) \n",
    "- Root mean squared logarithmic error (RMSLE)  \n",
    "- Mean percentage error (MPE)  \n",
    "- Mean absolute percentage error (MAPE)  \n",
    "- R2  Knowing about\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a **tpr** and **fpr** value for each threshold. \n",
    "\n",
    "Plot TPR on the y-axis and FPR  on the x-axis. This curve is also known as the **Receiver Operating Characteristic (ROC)**. And  if we calculate the area under this ROC curve, we are calculating another metric  which is used very often when you have a dataset which has skewed binary targets.  This metric is known as the **Area Under ROC Curve** or Area Under Curve or  just simply **AUC**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important metric you should learn after learning AUC is log loss. In case  of a binary classification problem, we define log loss as:  \n",
    "\n",
    "**Log Loss = - 1.0 * (target * log(prediction) + (1 - target) * log(1 - prediction))**\n",
    "\n",
    "Where target is either 0 or 1 and prediction is a probability of a sample belonging  to class 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate  precision and recall for each class in a **multi-class classification** problem.  \n",
    "There are three different ways to calculate this which might get confusing from time  to time. Letâ€™s assume we are interested in precision first. We know that precision  depends on true positives and false positives. \n",
    "- **Macro averaged precision**: calculate precision for all classes individually  and then average them  \n",
    "- **Micro averaged precision**: calculate class wise true positive and false  positive and then use that to calculate overall precision  \n",
    "- **Weighted precision**: same as macro but in this case, it is weighted average  depending on the number of items in each class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then comes another type of classification problem called **multi-label classification**. In multi-label classification, each sample can have one or more  classes associated with it.\n",
    "\n",
    "The metrics for this type of classification problem are a bit different. Some suitable  and most common metrics are:\n",
    "- Precision at k (P@k)\n",
    "- Average precision at k (AP@k) \n",
    "- Mean average precision at k (MAP@k)  \n",
    "- Log loss\n",
    "\n",
    "If you have a list of original classes for a given  sample and list of predicted classes for the same, precision is defined as the number  of hits in the predicted list considering only top-k predictions, divided by k. \n",
    "\n",
    "we have average precision at k or AP@k. AP@k is calculated using P@k.  For example, if we have to calculate AP@3, we calculate P@1, P@2 and P@3 and  then divide the sum by 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
