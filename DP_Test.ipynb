{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Payments Dev Notebook - v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### INV - PAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:45:32.907979Z",
     "start_time": "2020-04-21T10:45:31.921407Z"
    },
    "cell_status": {
     "execute_time": {
      "duration": 2275.73583984375,
      "end_time": 1587033781924.255
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SwaritSankule\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# DP Org\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import re\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import display, HTML\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# pyspark \n",
    "import pyspark\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.sql.functions import (\n",
    "    col, max as max_, struct, monotonically_increasing_id, regexp_replace,\n",
    "    ltrim, rtrim, trim, split, expr, when, concat, lit, create_map, month, year,\n",
    "    col,to_date, udf, unix_timestamp, from_unixtime, expr, log, log10, log2, abs,lower,datediff, concat_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:45:39.570046Z",
     "start_time": "2020-04-21T10:45:32.948650Z"
    }
   },
   "outputs": [],
   "source": [
    "# starting session\n",
    "spark = SparkSession.builder.appName(\"DP_INV_PAY\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "db_url = \"jdbc:sqlserver://azrdwhlantern.database.windows.net:1433;databaseName=azrdwhlantern\"\n",
    "db_name = 'azrdwhlantern'\n",
    "db_user = \"swarit\"\n",
    "db_pwd = \"Welcome123\"\n",
    "\n",
    "def read_dwh_query(query):\n",
    "    query = \"(\" + query + \") tmp\"\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url) \\\n",
    "        .option(\"dbtable\", query) \\\n",
    "        .option(\"user\", db_user) \\\n",
    "        .option(\"password\", db_pwd) \\\n",
    "        .load()\n",
    "    \n",
    "    return jdbcDF\n",
    "\n",
    "# columns definition\n",
    "payments_data_fields = ['Id', 'source_system', 'zone', 'company_code','vendor_id', 'invoice_reference_number',\n",
    "                        'invoice_date', 'total_invoice_amount', 'payment_accounting_document_number', \n",
    "                       'vendor_is_intercompany', 'vendor_is_employee']\n",
    "\n",
    "# for source system\n",
    "# query = \"SELECT %s\" % ', '.join(str(item) for item in payments_data_fields[1:]) +  f\" from import.invoice_payment_dp where source_system  = '{source_system}'\"\n",
    "\n",
    "# full data\n",
    "# query = \"SELECT %s\" % ', '.join(str(item) for item in payments_data_fields[1:]) +  f\" from import.invoice_payment_dp\"\n",
    "\n",
    "# reading payments data\n",
    "payments_data = read_dwh_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Sample Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:23.217110Z",
     "start_time": "2020-04-21T10:46:22.760852Z"
    }
   },
   "outputs": [],
   "source": [
    "# reading sample excel\n",
    "payments_data = spark.read.csv('dp_sample_data.csv',inferSchema=True,header=True)\n",
    "\n",
    "# Change data type\n",
    "payments_data = payments_data.withColumn(\"vendor_id\", payments_data[\"vendor_id\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Pre - Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:23.974806Z",
     "start_time": "2020-04-21T10:46:23.701934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning done for >> company_code\n",
      "cleaning done for >> vendor_id\n",
      "cleaning done for >> invoice_reference_number\n",
      "cleaning done for >> invoice_date\n",
      "cleaning done for >> total_invoice_amount\n",
      "+------------+----------+------------------------+------------+--------------------+---+\n",
      "|company_code| vendor_id|invoice_reference_number|invoice_date|total_invoice_amount| id|\n",
      "+------------+----------+------------------------+------------+--------------------+---+\n",
      "|       erp08|    355186|             IRN_4752_01|    22102019|                 350|  1|\n",
      "|       erp08|    355186|             IRN_4752_01|    22102019|                 350|  2|\n",
      "|       erp08|    355186|             IRN_4752_01|    22102019|                 350|  3|\n",
      "|       erp09|    355185|             IRN_4752_01|    22102019|                 350|  4|\n",
      "|       erp09|    355185|             IRN_4752_01|    22102019|                 350|  5|\n",
      "|       erp08|  355456T2|               IRN_98_01|    22102019|                2000|  6|\n",
      "|       erp08|  355456T2|               IRN_98_01|    22102019|               10000|  7|\n",
      "|       erp08|355457T1T3|                IRN_1002|    24102019|               13000|  8|\n",
      "|       erp08|355457T1T3|                IRN_1003|    24102019|               13000|  9|\n",
      "|       erp08|  355458T4|                 IRN_297|    01042020|                3000| 10|\n",
      "|       erp08|  355458T4|                 IRN_297|    02042020|                3000| 11|\n",
      "+------------+----------+------------------------+------------+--------------------+---+\n",
      "\n",
      "Part(1/6) - Data Processing Done!\n"
     ]
    }
   ],
   "source": [
    "# remove non alpha numerics\n",
    "for col_iter in payments_data.columns:\n",
    "    payments_data = payments_data.withColumn(col_iter,regexp_replace(col_iter, r'[^a-zA-Z0-9_]', ''))\n",
    "    print(\"cleaning done for >> {}\".format(col_iter))\n",
    "\n",
    "# adding ID column to payments data\n",
    "payments_data = payments_data.select(\"*\").withColumn(\"id\", monotonically_increasing_id() + 1)\n",
    "\n",
    "payments_data.show()\n",
    "\n",
    "print(\"Part(1/6) - Data Processing Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating ATH and Test based dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:32.647812Z",
     "start_time": "2020-04-21T10:46:32.642711Z"
    }
   },
   "outputs": [],
   "source": [
    "# payments_data_org = payments_data\n",
    "payments_data = payments_data_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:33.903657Z",
     "start_time": "2020-04-21T10:46:33.830580Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating matchkey for exact match\n",
    "payments_data = payments_data.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                         payments_data.company_code,\n",
    "                                                         payments_data.vendor_id,\n",
    "                                                         payments_data.invoice_date,\n",
    "                                                         payments_data.invoice_reference_number,\n",
    "                                                         payments_data.total_invoice_amount))\n",
    "\n",
    "\n",
    "# duplicated rows\n",
    "temp_df = payments_data.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "# Change data type\n",
    "temp_df = temp_df.withColumn(\"count\", temp_df[\"count\"].cast(StringType()))\n",
    "\n",
    "# reform temp df to append later\n",
    "payments_data = payments_data.join(temp_df, on=['match_key'], how='left')\n",
    "\n",
    "# Creating Separate dataframes\n",
    "payments_df_ath = payments_data.filter(~temp_df[\"count\"].isNull())\n",
    "payments_df_test = payments_data.filter(temp_df[\"count\"].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:36.166903Z",
     "start_time": "2020-04-21T10:46:34.708456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+---------+------------------------+------------+--------------------+---+-----+\n",
      "|           match_key|company_code|vendor_id|invoice_reference_number|invoice_date|total_invoice_amount| id|count|\n",
      "+--------------------+------------+---------+------------------------+------------+--------------------+---+-----+\n",
      "|erp08-355186-2210...|       erp08|   355186|             IRN_4752_01|    22102019|                 350|  1|    3|\n",
      "|erp08-355186-2210...|       erp08|   355186|             IRN_4752_01|    22102019|                 350|  2|    3|\n",
      "|erp08-355186-2210...|       erp08|   355186|             IRN_4752_01|    22102019|                 350|  3|    3|\n",
      "|erp09-355185-2210...|       erp09|   355185|             IRN_4752_01|    22102019|                 350|  4|    2|\n",
      "|erp09-355185-2210...|       erp09|   355185|             IRN_4752_01|    22102019|                 350|  5|    2|\n",
      "+--------------------+------------+---------+------------------------+------------+--------------------+---+-----+\n",
      "\n",
      "+--------------------+------------+----------+------------------------+------------+--------------------+---+-----+\n",
      "|           match_key|company_code| vendor_id|invoice_reference_number|invoice_date|total_invoice_amount| id|count|\n",
      "+--------------------+------------+----------+------------------------+------------+--------------------+---+-----+\n",
      "|erp08-355456T2-22...|       erp08|  355456T2|               IRN_98_01|    22102019|                2000|  6| null|\n",
      "|erp08-355456T2-22...|       erp08|  355456T2|               IRN_98_01|    22102019|               10000|  7| null|\n",
      "|erp08-355457T1T3-...|       erp08|355457T1T3|                IRN_1002|    24102019|               13000|  8| null|\n",
      "|erp08-355457T1T3-...|       erp08|355457T1T3|                IRN_1003|    24102019|               13000|  9| null|\n",
      "|erp08-355458T4-01...|       erp08|  355458T4|                 IRN_297|    01042020|                3000| 10| null|\n",
      "|erp08-355458T4-02...|       erp08|  355458T4|                 IRN_297|    02042020|                3000| 11| null|\n",
      "+--------------------+------------+----------+------------------------+------------+--------------------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payments_df_ath.show()\n",
    "payments_df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Payments DF ATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:38.565943Z",
     "start_time": "2020-04-21T10:46:38.397976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1\n",
      "T2\n",
      "T3\n",
      "T4\n"
     ]
    }
   ],
   "source": [
    "# drop count\n",
    "payments_df_ath = payments_df_ath.drop('count')\n",
    "\n",
    "# cluster number\n",
    "payments_df_ath = payments_df_ath.withColumn(\"cluster_num\", temp_df[\"match_key\"])\n",
    "\n",
    "# match percent\n",
    "payments_df_ath = payments_df_ath.withColumn(\"match_percent\", lit(100))\n",
    "\n",
    "# for each test\n",
    "for test_iter in [\"T1\", \"T2\", \"T3\", \"T4\"]:\n",
    "    print(test_iter)\n",
    "    payments_df_ath = payments_df_ath.withColumn(\"{}_%\".format(test_iter), lit(100))\n",
    "    payments_df_ath = payments_df_ath.withColumn(\"{}_flag\".format(test_iter), lit(1))\n",
    "    payments_df_ath = payments_df_ath.withColumn(\"{}_cluster_num\".format(test_iter), payments_df_ath['cluster_num'])\n",
    "\n",
    "payments_df_ath = payments_df_ath.drop('match_key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Payments DF TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:39.778913Z",
     "start_time": "2020-04-21T10:46:39.770435Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop match key\n",
    "payments_df_test = payments_df_test.drop('match_key', 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T12:04:17.909770Z",
     "start_time": "2020-04-20T12:04:17.840953Z"
    }
   },
   "outputs": [],
   "source": [
    "## Test Definition\n",
    "# \"Overall\" : ['vendor_id', 'invoice_reference_number', 'invoice_date', 'total_invoice_amount'],\n",
    "# \"T1\" : ['vendor_id', 'invoice_date', 'total_invoice_amount'],\n",
    "# \"T2\" : ['invoice_date', 'invoice_reference_number'],\n",
    "# \"T3\" : ['invoice_date', 'total_invoice_amount'],\n",
    "# \"T4\" : ['vendor_id','invoice_reference_number','total_invoice_amount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to call for each test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:41.650335Z",
     "start_time": "2020-04-21T10:46:41.628395Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_cluster_creator(test_name):\n",
    "\n",
    "    test_list  = [\"T1\", \"T2\", \"T3\", \"T4\"]\n",
    "    \n",
    "    # condition to define temp payment df for each test\n",
    "    if(test_name == 'T1'):\n",
    "        test_list.remove(\"T1\")\n",
    "        payments_df_test_temp = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                         payments_df_test.company_code,\n",
    "                                                         payments_df_test.vendor_id,\n",
    "                                                         payments_df_test.invoice_date,\n",
    "                                                         payments_df_test.total_invoice_amount))\n",
    "        \n",
    "    elif(test_name == 'T2'):\n",
    "        test_list.remove(\"T2\")\n",
    "        payments_df_test_temp = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                             payments_df_test.company_code,\n",
    "                                                             payments_df_test.invoice_date,\n",
    "                                                             payments_df_test.invoice_reference_number))\n",
    "\n",
    "    elif(test_name == 'T3'):\n",
    "        test_list.remove(\"T3\")\n",
    "        payments_df_test_temp = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                             payments_df_test.company_code,\n",
    "                                                             payments_df_test.invoice_date,\n",
    "                                                             payments_df_test.total_invoice_amount))\n",
    "        \n",
    "    elif(test_name == 'T4'):\n",
    "        test_list.remove(\"T4\")\n",
    "        payments_df_test_temp = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                             payments_df_test.company_code,\n",
    "                                                             payments_df_test.vendor_id,\n",
    "                                                             payments_df_test.invoice_reference_number,\n",
    "                                                             payments_df_test.total_invoice_amount))\n",
    "        \n",
    "    \n",
    "    # duplicated rows\n",
    "    temp_df = payments_df_test_temp.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "    # Change data type\n",
    "    temp_df = temp_df.withColumn(\"count\", temp_df[\"count\"].cast(StringType()))\n",
    "\n",
    "    # reform temp df to append later\n",
    "    temp_df = payments_df_test_temp.join(temp_df, on=['match_key'], how='left')\n",
    "\n",
    "    # Creating Separate dataframes\n",
    "    temp_df = temp_df.filter(~temp_df[\"count\"].isNull())\n",
    "\n",
    "    # cluster number\n",
    "    temp_df = temp_df.withColumn(\"cluster_num\", temp_df[\"match_key\"])\n",
    "\n",
    "    # drop count and match key\n",
    "    temp_df = temp_df.drop('count', 'match_key')\n",
    "\n",
    "    # default match percent\n",
    "    temp_df = temp_df.withColumn(\"match_percent\", lit(0))\n",
    "\n",
    "    # Cluster number and T1 flag\n",
    "    temp_df = temp_df.withColumn(\"{}_flag\".format(test_name), lit(1))\n",
    "    temp_df = temp_df.withColumn(\"{}_%\".format(test_name), lit(100))\n",
    "    temp_df = temp_df.withColumn(\"{}_cluster_num\".format(test_name), temp_df['cluster_num'])\n",
    "\n",
    "\n",
    "    # for each test\n",
    "    for test_iter in test_list:\n",
    "        temp_df = temp_df.withColumn(\"{}_flag\".format(test_iter), lit(0))\n",
    "        temp_df = temp_df.withColumn(\"{}_%\".format(test_iter), lit(0))\n",
    "        temp_df = temp_df.withColumn(\"{}_cluster_num\".format(test_iter), lit(\"-\"))\n",
    "\n",
    "    # aligning columns as per payments_df_ath\n",
    "    temp_df = temp_df.select(payments_df_ath.columns)\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:46:49.346236Z",
     "start_time": "2020-04-21T10:46:42.476478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1 count => 2\n",
      "T2 count => 2\n",
      "T3 count => 2\n",
      "T4 count => 2\n"
     ]
    }
   ],
   "source": [
    "T1_df = test_cluster_creator(\"T1\")\n",
    "print(\"T1 count => {}\".format(T1_df.count()))\n",
    "\n",
    "T2_df = test_cluster_creator(\"T2\")\n",
    "print(\"T2 count => {}\".format(T2_df.count()))\n",
    "\n",
    "T3_df = test_cluster_creator(\"T3\")\n",
    "print(\"T3 count => {}\".format(T3_df.count()))\n",
    "\n",
    "T4_df = test_cluster_creator(\"T4\")\n",
    "print(\"T4 count => {}\".format(T4_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:47:14.869118Z",
     "start_time": "2020-04-21T10:47:14.265552Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------------+------------+--------------------+---+\n",
      "|company_code| vendor_id|invoice_reference_number|invoice_date|total_invoice_amount| id|\n",
      "+------------+----------+------------------------+------------+--------------------+---+\n",
      "|       erp08|  355456T2|               IRN_98_01|    22102019|                2000|  6|\n",
      "|       erp08|  355456T2|               IRN_98_01|    22102019|               10000|  7|\n",
      "|       erp08|355457T1T3|                IRN_1002|    24102019|               13000|  8|\n",
      "|       erp08|355457T1T3|                IRN_1003|    24102019|               13000|  9|\n",
      "|       erp08|  355458T4|                 IRN_297|    01042020|                3000| 10|\n",
      "|       erp08|  355458T4|                 IRN_297|    02042020|                3000| 11|\n",
      "+------------+----------+------------------------+------------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payments_df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T10:47:18.852499Z",
     "start_time": "2020-04-21T10:47:18.778696Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge all 4 DF together\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "test_df_append = unionAll(T1_df, T2_df, T3_df, T4_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T11:32:30.204176Z",
     "start_time": "2020-04-21T11:32:30.174258Z"
    }
   },
   "outputs": [],
   "source": [
    "# appending to ATH\n",
    "final_df = unionAll(payments_df_ath,test_df_append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T11:32:39.847709Z",
     "start_time": "2020-04-21T11:32:35.831710Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_code</th>\n",
       "      <th>vendor_id</th>\n",
       "      <th>invoice_reference_number</th>\n",
       "      <th>invoice_date</th>\n",
       "      <th>total_invoice_amount</th>\n",
       "      <th>id</th>\n",
       "      <th>cluster_num</th>\n",
       "      <th>match_percent</th>\n",
       "      <th>T1_%</th>\n",
       "      <th>T1_flag</th>\n",
       "      <th>T1_cluster_num</th>\n",
       "      <th>T2_%</th>\n",
       "      <th>T2_flag</th>\n",
       "      <th>T2_cluster_num</th>\n",
       "      <th>T3_%</th>\n",
       "      <th>T3_flag</th>\n",
       "      <th>T3_cluster_num</th>\n",
       "      <th>T4_%</th>\n",
       "      <th>T4_flag</th>\n",
       "      <th>T4_cluster_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355186</td>\n",
       "      <td>IRN_4752_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>350</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355186</td>\n",
       "      <td>IRN_4752_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>350</td>\n",
       "      <td>2</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355186</td>\n",
       "      <td>IRN_4752_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>350</td>\n",
       "      <td>3</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355186-22102019-IRN_4752_01-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>erp09</td>\n",
       "      <td>355185</td>\n",
       "      <td>IRN_4752_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>350</td>\n",
       "      <td>4</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>erp09</td>\n",
       "      <td>355185</td>\n",
       "      <td>IRN_4752_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>350</td>\n",
       "      <td>5</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp09-355185-22102019-IRN_4752_01-350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355457T1T3</td>\n",
       "      <td>IRN_1002</td>\n",
       "      <td>24102019</td>\n",
       "      <td>13000</td>\n",
       "      <td>8</td>\n",
       "      <td>erp08-355457T1T3-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355457T1T3-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355457T1T3</td>\n",
       "      <td>IRN_1003</td>\n",
       "      <td>24102019</td>\n",
       "      <td>13000</td>\n",
       "      <td>9</td>\n",
       "      <td>erp08-355457T1T3-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355457T1T3-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355456T2</td>\n",
       "      <td>IRN_98_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>2000</td>\n",
       "      <td>6</td>\n",
       "      <td>erp08-22102019-IRN_98_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-22102019-IRN_98_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355456T2</td>\n",
       "      <td>IRN_98_01</td>\n",
       "      <td>22102019</td>\n",
       "      <td>10000</td>\n",
       "      <td>7</td>\n",
       "      <td>erp08-22102019-IRN_98_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-22102019-IRN_98_01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355457T1T3</td>\n",
       "      <td>IRN_1002</td>\n",
       "      <td>24102019</td>\n",
       "      <td>13000</td>\n",
       "      <td>8</td>\n",
       "      <td>erp08-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355457T1T3</td>\n",
       "      <td>IRN_1003</td>\n",
       "      <td>24102019</td>\n",
       "      <td>13000</td>\n",
       "      <td>9</td>\n",
       "      <td>erp08-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-24102019-13000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355458T4</td>\n",
       "      <td>IRN_297</td>\n",
       "      <td>01042020</td>\n",
       "      <td>3000</td>\n",
       "      <td>10</td>\n",
       "      <td>erp08-355458T4-IRN_297-3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355458T4-IRN_297-3000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>erp08</td>\n",
       "      <td>355458T4</td>\n",
       "      <td>IRN_297</td>\n",
       "      <td>02042020</td>\n",
       "      <td>3000</td>\n",
       "      <td>11</td>\n",
       "      <td>erp08-355458T4-IRN_297-3000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>erp08-355458T4-IRN_297-3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_code   vendor_id invoice_reference_number invoice_date  \\\n",
       "0         erp08      355186              IRN_4752_01     22102019   \n",
       "1         erp08      355186              IRN_4752_01     22102019   \n",
       "2         erp08      355186              IRN_4752_01     22102019   \n",
       "3         erp09      355185              IRN_4752_01     22102019   \n",
       "4         erp09      355185              IRN_4752_01     22102019   \n",
       "5         erp08  355457T1T3                 IRN_1002     24102019   \n",
       "6         erp08  355457T1T3                 IRN_1003     24102019   \n",
       "7         erp08    355456T2                IRN_98_01     22102019   \n",
       "8         erp08    355456T2                IRN_98_01     22102019   \n",
       "9         erp08  355457T1T3                 IRN_1002     24102019   \n",
       "10        erp08  355457T1T3                 IRN_1003     24102019   \n",
       "11        erp08    355458T4                  IRN_297     01042020   \n",
       "12        erp08    355458T4                  IRN_297     02042020   \n",
       "\n",
       "   total_invoice_amount  id                            cluster_num  \\\n",
       "0                   350   1  erp08-355186-22102019-IRN_4752_01-350   \n",
       "1                   350   2  erp08-355186-22102019-IRN_4752_01-350   \n",
       "2                   350   3  erp08-355186-22102019-IRN_4752_01-350   \n",
       "3                   350   4  erp09-355185-22102019-IRN_4752_01-350   \n",
       "4                   350   5  erp09-355185-22102019-IRN_4752_01-350   \n",
       "5                 13000   8        erp08-355457T1T3-24102019-13000   \n",
       "6                 13000   9        erp08-355457T1T3-24102019-13000   \n",
       "7                  2000   6               erp08-22102019-IRN_98_01   \n",
       "8                 10000   7               erp08-22102019-IRN_98_01   \n",
       "9                 13000   8                   erp08-24102019-13000   \n",
       "10                13000   9                   erp08-24102019-13000   \n",
       "11                 3000  10            erp08-355458T4-IRN_297-3000   \n",
       "12                 3000  11            erp08-355458T4-IRN_297-3000   \n",
       "\n",
       "    match_percent  T1_%  T1_flag                         T1_cluster_num  T2_%  \\\n",
       "0             100   100        1  erp08-355186-22102019-IRN_4752_01-350   100   \n",
       "1             100   100        1  erp08-355186-22102019-IRN_4752_01-350   100   \n",
       "2             100   100        1  erp08-355186-22102019-IRN_4752_01-350   100   \n",
       "3             100   100        1  erp09-355185-22102019-IRN_4752_01-350   100   \n",
       "4             100   100        1  erp09-355185-22102019-IRN_4752_01-350   100   \n",
       "5               0   100        1        erp08-355457T1T3-24102019-13000     0   \n",
       "6               0   100        1        erp08-355457T1T3-24102019-13000     0   \n",
       "7               0     0        0                                      -   100   \n",
       "8               0     0        0                                      -   100   \n",
       "9               0     0        0                                      -     0   \n",
       "10              0     0        0                                      -     0   \n",
       "11              0     0        0                                      -     0   \n",
       "12              0     0        0                                      -     0   \n",
       "\n",
       "    T2_flag                         T2_cluster_num  T3_%  T3_flag  \\\n",
       "0         1  erp08-355186-22102019-IRN_4752_01-350   100        1   \n",
       "1         1  erp08-355186-22102019-IRN_4752_01-350   100        1   \n",
       "2         1  erp08-355186-22102019-IRN_4752_01-350   100        1   \n",
       "3         1  erp09-355185-22102019-IRN_4752_01-350   100        1   \n",
       "4         1  erp09-355185-22102019-IRN_4752_01-350   100        1   \n",
       "5         0                                      -     0        0   \n",
       "6         0                                      -     0        0   \n",
       "7         1               erp08-22102019-IRN_98_01     0        0   \n",
       "8         1               erp08-22102019-IRN_98_01     0        0   \n",
       "9         0                                      -   100        1   \n",
       "10        0                                      -   100        1   \n",
       "11        0                                      -     0        0   \n",
       "12        0                                      -     0        0   \n",
       "\n",
       "                           T3_cluster_num  T4_%  T4_flag  \\\n",
       "0   erp08-355186-22102019-IRN_4752_01-350   100        1   \n",
       "1   erp08-355186-22102019-IRN_4752_01-350   100        1   \n",
       "2   erp08-355186-22102019-IRN_4752_01-350   100        1   \n",
       "3   erp09-355185-22102019-IRN_4752_01-350   100        1   \n",
       "4   erp09-355185-22102019-IRN_4752_01-350   100        1   \n",
       "5                                       -     0        0   \n",
       "6                                       -     0        0   \n",
       "7                                       -     0        0   \n",
       "8                                       -     0        0   \n",
       "9                    erp08-24102019-13000     0        0   \n",
       "10                   erp08-24102019-13000     0        0   \n",
       "11                                      -   100        1   \n",
       "12                                      -   100        1   \n",
       "\n",
       "                           T4_cluster_num  \n",
       "0   erp08-355186-22102019-IRN_4752_01-350  \n",
       "1   erp08-355186-22102019-IRN_4752_01-350  \n",
       "2   erp08-355186-22102019-IRN_4752_01-350  \n",
       "3   erp09-355185-22102019-IRN_4752_01-350  \n",
       "4   erp09-355185-22102019-IRN_4752_01-350  \n",
       "5                                       -  \n",
       "6                                       -  \n",
       "7                                       -  \n",
       "8                                       -  \n",
       "9                                       -  \n",
       "10                                      -  \n",
       "11            erp08-355458T4-IRN_297-3000  \n",
       "12            erp08-355458T4-IRN_297-3000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T09:37:07.400117Z",
     "start_time": "2020-04-21T09:37:07.216001Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"T1 DF\")\n",
    "payments_df_test = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                     payments_df_test.company_code,\n",
    "                                                     payments_df_test.vendor_id,\n",
    "                                                     payments_df_test.invoice_date,\n",
    "                                                     payments_df_test.total_invoice_amount))\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T1 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# Change data type\n",
    "temp_df_T1 = temp_df_T1.withColumn(\"count\", temp_df_T1[\"count\"].cast(StringType()))\n",
    "\n",
    "# reform temp df to append later\n",
    "temp_df_T1 = payments_df_test.join(temp_df_T1, on=['match_key'], how='left')\n",
    "\n",
    "# Creating Separate dataframes\n",
    "temp_df_T1 = temp_df_T1.filter(~temp_df_T1[\"count\"].isNull())\n",
    "\n",
    "# cluster number\n",
    "temp_df_T1 = temp_df_T1.withColumn(\"cluster_num\", temp_df_T1[\"match_key\"])\n",
    "\n",
    "# drop count and match key\n",
    "temp_df_T1 = temp_df_T1.drop('count', 'match_key')\n",
    "\n",
    "# default match percent\n",
    "temp_df_T1 = temp_df_T1.withColumn(\"match_percent\", lit(0))\n",
    "\n",
    "# Cluster number and T1 flag\n",
    "temp_df_T1 = temp_df_T1.withColumn(\"T1_flag\", lit(1))\n",
    "temp_df_T1 = temp_df_T1.withColumn(\"T1_%\", lit(100))\n",
    "temp_df_T1 = temp_df_T1.withColumn(\"T1_cluster_num\", temp_df_T1['cluster_num'])\n",
    "\n",
    "\n",
    "# for each test\n",
    "for test_iter in [\"T2\", \"T3\", \"T4\"]:\n",
    "    temp_df_T1 = temp_df_T1.withColumn(\"{}_flag\".format(test_iter), lit(0))\n",
    "    temp_df_T1 = temp_df_T1.withColumn(\"{}_%\".format(test_iter), lit(0))\n",
    "    temp_df_T1 = temp_df_T1.withColumn(\"{}_cluster_num\".format(test_iter), lit(\"-\"))\n",
    "    \n",
    "temp_df_T1 = temp_df_T1.select(payments_df_ath.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T09:16:23.172323Z",
     "start_time": "2020-04-21T09:16:22.989173Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"T2 DF\")\n",
    "payments_df_test = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                     payments_df_test.company_code,\n",
    "                                                     payments_df_test.invoice_date,\n",
    "                                                     payments_df_test.invoice_reference_number))\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T2 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T2 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# Change data type\n",
    "temp_df_T2 = temp_df_T2.withColumn(\"count\", temp_df_T2[\"count\"].cast(StringType()))\n",
    "\n",
    "# reform temp df to append later\n",
    "temp_df_T2 = payments_df_test.join(temp_df_T2, on=['match_key'], how='left')\n",
    "\n",
    "# Creating Separate dataframes\n",
    "temp_df_T2 = temp_df_T2.filter(~temp_df_T2[\"count\"].isNull())\n",
    "\n",
    "# cluster number\n",
    "temp_df_T2 = temp_df_T2.withColumn(\"cluster_num\", temp_df_T2[\"match_key\"])\n",
    "\n",
    "# drop count and match key\n",
    "temp_df_T2 = temp_df_T2.drop('count', 'match_key')\n",
    "\n",
    "# default match percent\n",
    "temp_df_T2 = temp_df_T2.withColumn(\"match_percent\", lit(0))\n",
    "\n",
    "# Cluster number and T1 flag\n",
    "temp_df_T2 = temp_df_T2.withColumn(\"T2_flag\", lit(1))\n",
    "temp_df_T2 = temp_df_T2.withColumn(\"T2_cluster_num\", temp_df_T2['cluster_num'])\n",
    "\n",
    "\n",
    "# for each test\n",
    "for test_iter in [\"TT\", \"T3\", \"T4\"]:\n",
    "    temp_df_T2 = temp_df_T2.withColumn(\"{}_flag\".format(test_iter), lit(0))\n",
    "    temp_df_T2 = temp_df_T2.withColumn(\"{}_cluster_num\".format(test_iter), lit(\"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T09:18:16.747717Z",
     "start_time": "2020-04-21T09:18:16.464210Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"T3 DF\")\n",
    "payments_df_test = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                     payments_df_test.company_code,\n",
    "                                                     payments_df_test.invoice_date,\n",
    "                                                     payments_df_test.total_invoice_amount))\n",
    "\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T3 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T3 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# Change data type\n",
    "temp_df_T3 = temp_df_T3.withColumn(\"count\", temp_df_T3[\"count\"].cast(StringType()))\n",
    "\n",
    "# reform temp df to append later\n",
    "temp_df_T3 = payments_df_test.join(temp_df_T3, on=['match_key'], how='left')\n",
    "\n",
    "# Creating Separate dataframes\n",
    "temp_df_T3 = temp_df_T3.filter(~temp_df_T3[\"count\"].isNull())\n",
    "\n",
    "# cluster number\n",
    "temp_df_T3 = temp_df_T3.withColumn(\"cluster_num\", temp_df_T3[\"match_key\"])\n",
    "\n",
    "# drop count and match key\n",
    "temp_df_T3 = temp_df_T3.drop('count', 'match_key')\n",
    "\n",
    "# default match percent\n",
    "temp_df_T3 = temp_df_T3.withColumn(\"match_percent\", lit(0))\n",
    "\n",
    "# Cluster number and T1 flag\n",
    "temp_df_T3 = temp_df_T3.withColumn(\"T3_flag\", lit(1))\n",
    "temp_df_T3 = temp_df_T3.withColumn(\"T3_%\", lit(0))\n",
    "temp_df_T3 = temp_df_T3.withColumn(\"T3_cluster_num\", temp_df_T3['cluster_num'])\n",
    "\n",
    "\n",
    "# for each test\n",
    "for test_iter in [\"TT\", \"T2\", \"T4\"]:\n",
    "    temp_df_T3 = temp_df_T3.withColumn(\"{}_flag\".format(test_iter), lit(0))\n",
    "    temp_df_T3 = temp_df_T3.withColumn(\"{}_cluster_num\".format(test_iter), lit(\"-\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T09:19:41.174345Z",
     "start_time": "2020-04-21T09:19:40.997418Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"T4 DF\")\n",
    "payments_df_test = payments_df_test.withColumn(\"match_key\",concat_ws('-', \n",
    "                                                     payments_df_test.company_code,\n",
    "                                                     payments_df_test.vendor_id,\n",
    "                                                     payments_df_test.invoice_reference_number,\n",
    "                                                     payments_df_test.total_invoice_amount))\n",
    "\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T4 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# duplicated rows\n",
    "temp_df_T4 = payments_df_test.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "\n",
    "# Change data type\n",
    "temp_df_T4 = temp_df_T4.withColumn(\"count\", temp_df_T4[\"count\"].cast(StringType()))\n",
    "\n",
    "# reform temp df to append later\n",
    "temp_df_T4 = payments_df_test.join(temp_df_T4, on=['match_key'], how='left')\n",
    "\n",
    "# Creating Separate dataframes\n",
    "temp_df_T4 = temp_df_T4.filter(~temp_df_T4[\"count\"].isNull())\n",
    "\n",
    "# cluster number\n",
    "temp_df_T4 = temp_df_T4.withColumn(\"cluster_num\", temp_df_T4[\"match_key\"])\n",
    "\n",
    "# drop count and match key\n",
    "temp_df_T4 = temp_df_T4.drop('count', 'match_key')\n",
    "\n",
    "# default match percent\n",
    "temp_df_T4 = temp_df_T4.withColumn(\"match_percent\", lit(0))\n",
    "\n",
    "# Cluster number and T1 flag\n",
    "temp_df_T4 = temp_df_T4.withColumn(\"T4_flag\", lit(1))\n",
    "temp_df_T4 = temp_df_T4.withColumn(\"T4_%\", lit(0))\n",
    "temp_df_T4 = temp_df_T4.withColumn(\"T4_cluster_num\", temp_df_T4['cluster_num'])\n",
    "\n",
    "\n",
    "# for each test\n",
    "for test_iter in [\"TT\", \"T2\", \"T3\"]:\n",
    "    temp_df_T4 = temp_df_T4.withColumn(\"{}_flag\".format(test_iter), lit(0))\n",
    "    temp_df_T4 = temp_df_T4.withColumn(\"{}_cluster_num\".format(test_iter), lit(\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T09:19:49.002631Z",
     "start_time": "2020-04-21T09:19:47.781027Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df_T4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T09:20:29.094055Z",
     "start_time": "2020-04-21T09:20:29.034658Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge all 4 DF together\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "temp_df_test = unionAll(temp_df_T1, temp_df_T2, temp_df_T3, temp_df_T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T15:25:31.447449Z",
     "start_time": "2020-04-20T15:25:30.800095Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T12:04:17.909770Z",
     "start_time": "2020-04-20T12:04:17.840953Z"
    }
   },
   "outputs": [],
   "source": [
    "# duplicated rows\n",
    "temp_df = payments_df_ath.groupBy(\"match_key\").count().filter(\"count>1\")\n",
    "# Change data type\n",
    "temp_df = temp_df.withColumn(\"count\", temp_df[\"count\"].cast(StringType()))\n",
    "# reform temp df to append later\n",
    "temp_df = payments_df_ath.join(temp_df, on=['match_key'], how='left')\n",
    "# removing Nulls\n",
    "temp_df = temp_df.filter(~temp_df[\"count\"].isNull())\n",
    "# drop count\n",
    "temp_df = temp_df.drop('count')\n",
    "temp_df = temp_df.withColumn(\"cluster_num\", temp_df[\"match_key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T15:20:54.641434Z",
     "start_time": "2020-04-20T15:20:54.558701Z"
    }
   },
   "outputs": [],
   "source": [
    "# match percent default to 0\n",
    "temp_df = temp_df.withColumn(\"match_percent\".format(test_iter), lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T12:04:19.698757Z",
     "start_time": "2020-04-20T12:04:19.039521Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"Overall\" : ['vendor_id', 'invoice_reference_number', 'invoice_date', 'total_invoice_amount'],\n",
    "             \"T1\" : ['invoice_date', 'total_invoice_amount', 'vendor_id'],\n",
    "            \"T2\" : ['invoice_date', 'invoice_reference_number'],\n",
    "            \"T3\" : ['invoice_date', 'total_invoice_amount'],\n",
    "            \"T4\" : ['total_invoice_amount', 'vendor_id','invoice_reference_number']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "**Cluster Numbers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-20T11:44:32.671961Z",
     "start_time": "2020-04-20T11:44:32.027558Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating cluster number\n",
    "indexer = StringIndexer(inputCol=\"match_key\", outputCol=\"cluster_num\")\n",
    "temp_df = indexer.fit(temp_df).transform(temp_df)\n",
    "temp_df = temp_df.withColumn(\"cluster_num\", temp_df[\"cluster_num\"]+1)\n",
    "temp_df = temp_df.withColumn(\"cluster_num\", temp_df[\"cluster_num\"].cast(StringType()))\n",
    "temp_df = temp_df.withColumn(\"cluster_num\",concat_ws('_',temp_df.company_code,temp_df.cluster_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Old Code - //remove later//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 45.744140625,
      "end_time": 1587033781984
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "db_url = \"jdbc:sqlserver://azrdwhlantern.database.windows.net:1433;databaseName=azrdwhlantern\"\n",
    "db_name = 'azrdwhlantern'\n",
    "db_user = \"swarit\"\n",
    "db_pwd = \"Welcome123\"\n",
    "\n",
    "def read_dwh_query(query):\n",
    "    query = \"(\" + query + \") tmp\"\n",
    "    jdbcDF = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url) \\\n",
    "        .option(\"dbtable\", query) \\\n",
    "        .option(\"user\", db_user) \\\n",
    "        .option(\"password\", db_pwd) \\\n",
    "        .load()\n",
    "    \n",
    "    return jdbcDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 63.8330078125,
      "end_time": 1587033782088.122
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_system = 'adopt_erp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3308.39208984375,
      "end_time": 1587033785426.925
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "payments_data_fields = ['Id', 'source_system', 'zone', 'company_code','vendor_id', 'invoice_reference_number',\n",
    "                        'invoice_date', 'total_invoice_amount', 'payment_accounting_document_number']\n",
    "\n",
    "# data for source system\n",
    "query = \"SELECT TOP 10000 %s\" % ', '.join(str(item) for item in payments_data_fields[1:]) +  \" from import.invoice_payment_dp where source_system  = '{}'\".format(source_system)\n",
    "\n",
    "# full data\n",
    "# query = \"SELECT %s\" % ', '.join(str(item) for item in payments_data_fields[1:]) +  f\" from import.invoice_payment_dp\"\n",
    "\n",
    "# reading payments data\n",
    "payments_data = read_dwh_query(query)\n",
    "\n",
    "# adding ID column to payments data\n",
    "payments_data = payments_data.select(\"*\").withColumn(\"id\", monotonically_increasing_id() + 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 11332.445068359375,
      "end_time": 1587033796770.142
     }
    },
    "hidden": true
   },
   "source": [
    "# shape\n",
    "(payments_data.count(), len(payments_data.columns))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 770.69091796875,
      "end_time": 1587029872510.885
     }
    },
    "hidden": true
   },
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "payments_data.select([count(when(isnan(c), c)).alias(c) for c in payments_data.columns]).show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 60.083984375,
      "end_time": 1587029478380.776
     }
    },
    "hidden": true
   },
   "source": [
    "payments_data.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 811.72119140625,
      "end_time": 1587029305382.107
     }
    },
    "hidden": true
   },
   "source": [
    "payments_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3302.864013671875,
      "end_time": 1587030086510.229
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_df = payments_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 93.08984375,
      "end_time": 1587030113740.983
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 3281.845947265625,
      "end_time": 1587030145829.183
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting back\n",
    "test_df2 = sqlContext.createDataFrame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----- \n",
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "source_system = 'adopt_erp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 751.40087890625,
      "end_time": 1587039433520.142
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "payments_data_fields = ['Id', 'source_system', 'zone', 'company_code','vendor_id', 'invoice_reference_number',\n",
    "                        'invoice_date', 'total_invoice_amount', 'payment_accounting_document_number',\n",
    "                       'vendor_is_intercompany', 'vendor_is_employee']\n",
    "\n",
    "# data for source system\n",
    "query = \"SELECT %s\" % ', '.join(str(item) for item in payments_data_fields[1:]) +  \" from import.invoice_payment_dp where source_system  = '{}'\".format(source_system)\n",
    "\n",
    "# full data\n",
    "# query = \"SELECT %s\" % ', '.join(str(item) for item in payments_data_fields[1:]) +  f\" from import.invoice_payment_dp\"\n",
    "\n",
    "# reading payments data\n",
    "payments_data = read_dwh_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 23417.866943359375,
      "end_time": 1587039458520.514
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "payments_data = payments_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 1263.489013671875,
      "end_time": 1587039459799.222
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# removing intercompany and employee transcations\n",
    "payments_data = payments_data.loc[(payments_data['vendor_is_intercompany'] != 'Y')]\n",
    "payments_data = payments_data.loc[(payments_data['vendor_is_employee'] != 'Y')]\n",
    "payments_data.drop(columns= ['vendor_is_intercompany', 'vendor_is_employee'], inplace = True)\n",
    "\n",
    "# Adding the Id column to the imported data\n",
    "payments_data = payments_data.reset_index(drop=False)\n",
    "payments_data.rename(columns={'index':'Id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 57621.73388671875,
      "end_time": 1587039560823.219
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "payments_data_fields = ['Id','source_system','zone', 'company_code','vendor_id', 'invoice_reference_number',\n",
    "                        'invoice_date', 'total_invoice_amount', 'payment_accounting_document_number']\n",
    "\n",
    "#######################################################################################\n",
    "############################# PRE - PROCESS PART ######################################\n",
    "#######################################################################################\n",
    "\n",
    "data = payments_data.copy()\n",
    "\n",
    "# Remove trailing zeros (problem based solution)\n",
    "def remove_trailing_zeros(x):\n",
    "    try:\n",
    "        # Convert to int\n",
    "        x = int(x)\n",
    "        # Convert back to str\n",
    "        x = str(x)\n",
    "        return x\n",
    "    except:\n",
    "        return str(x)\n",
    "\n",
    "for field in payments_data_fields:\n",
    "    data[field] = data[field].apply(remove_trailing_zeros)\n",
    "    data[field] = data[field].apply(lambda x: x.lstrip())\n",
    "    data[field] = data[field].apply(lambda x: x.rstrip())\n",
    "\n",
    "# Subset the data for the required columns\n",
    "# payments_data = payments_data[['Id', 'vendor_id', 'po_number', 'invoice_reference_number', 'invoice_date', 'total_invoice_amount']]\n",
    "data = data[payments_data_fields]\n",
    "\n",
    "data['vendor_id_length'] = data['vendor_id'].str.len()\n",
    "# print(\">>>>>>>>>>>>>>>>>>>>>>> VENDOR ID LENGTH <<<<<<<<<<<<<<<<<<<<<\")\n",
    "# print(data['vendor_id_length'].value_counts())\n",
    "# len(data)\n",
    "data = data[(data['vendor_id_length'] == 6) | (data['vendor_id_length'] == 7)]\n",
    "# len(data)\n",
    "if(len(data) == 0):\n",
    "    print('All records are filtered OUT on length of Vendor ID!! Check data source!!')\n",
    "    \n",
    "data = data.replace('<not provided>',np.nan)\n",
    "data = data.replace('[no vendor]',np.nan)\n",
    "#data = data.replace('None',np.nan)\n",
    "data['total_invoice_amount'] = data['total_invoice_amount'].replace(' ',np.nan).astype(float)\n",
    "\n",
    "# print(data.isna().sum())\n",
    "data = data.fillna('0')\n",
    "# print(data.isna().sum())\n",
    "\n",
    "\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "print('###############################################################################')\n",
    "print(\"Number of samples in imported dataset: \" + str(data.shape[0]))\n",
    "print(\"Number of fields in imported dataset: \" + str(data.shape[1]))\n",
    "print('###############################################################################')\n",
    "\n",
    "def preProcess(field):\n",
    "    column = data[field]    \n",
    "    column = column.map(lambda x: re.sub(r'[^\\x00-\\x7F]+',' ', str(x)))\n",
    "    column = column.apply(str)\n",
    "    column = column.map(lambda x: re.sub('  +', ' ', x))\n",
    "    column = column.map(lambda x: re.sub('\\n', ' ', x))\n",
    "    column = column.map(lambda x: x.strip())\n",
    "    column = column.map(lambda x: x.strip('\"'))\n",
    "    column = column.map(lambda x: x.strip(\"'\"))\n",
    "    column = column.map(lambda x: x.lower())\n",
    "    column = column.map(lambda x: re.sub('nan', ' ', x))\n",
    "    #column = column.map(lambda x: x.encode('unicode-escape').decode('utf-8'))\n",
    "    return column\n",
    "\n",
    "for field in list(data.columns):\n",
    "    print(\"Cleaning column: \" + field)\n",
    "    data[field] = preProcess(field) \n",
    "print(\"Done Cleaning Columns\")\n",
    "\n",
    "# data.apply(lambda x: (x == \"\").sum())\n",
    "data = data.loc[data['invoice_reference_number'] != \"\",]\n",
    "# data.isna().sum()\n",
    "\n",
    "print(\"No. of Data Points: {}\".format(len(data)))\n",
    "print(\"No. of Unique company codes: {}\".format(len(set(data['company_code']))))\n",
    "\n",
    "data['company_code'] = data['company_code'].replace('[^a-zA-Z0-9 ]', \"\", regex=True)\n",
    "data['vendor_id'] = data['vendor_id'].replace('[^a-zA-Z0-9 ]', \"\", regex=True)\n",
    "\n",
    "\n",
    "data['invoice_reference_number'] = data['invoice_reference_number'].replace('[^a-zA-Z0-9 ]', \"\", regex=True)\n",
    "data['invoice_date'] = data['invoice_date'].replace('[^a-zA-Z0-9 ]', \"\", regex=True)\n",
    "data['invoice_date'] = data['invoice_date'].str.replace(' 000000','')\n",
    "data = data[['Id','source_system','zone','company_code','vendor_id','invoice_reference_number','invoice_date','total_invoice_amount', 'payment_accounting_document_number']]\n",
    "data['match_key'] = data['vendor_id'] + \"-\" + data['invoice_reference_number'] + \"-\" + data['invoice_date'] + \"-\" + data['total_invoice_amount']\n",
    "\n",
    "\n",
    "print(\"Part(1/6) - Data Processing Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 19381.24609375,
      "end_time": 1587039618109.754
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "############################# ATH CREATION PART  ######################################\n",
    "#######################################################################################\n",
    "\n",
    "## Payment DF ATH\n",
    "\n",
    "payments_df_ath = data.copy()\n",
    "payments_df_ath = payments_df_ath.loc[:,['Id','company_code', 'vendor_id', 'invoice_reference_number','invoice_date', 'total_invoice_amount', 'payment_accounting_document_number']] # Added Id column to the list of columns retained\n",
    "payments_df_ath['cluster_num'] = '1'\n",
    "\n",
    "test_dict = {\"Overall\" : ['vendor_id', 'invoice_reference_number', 'invoice_date', 'total_invoice_amount'],\n",
    "             \"T1\" : ['invoice_date', 'total_invoice_amount', 'vendor_id'],\n",
    "             \"T2\" : ['invoice_date', 'invoice_reference_number'],\n",
    "             \"T3\" : ['invoice_date', 'total_invoice_amount'],\n",
    "             \"T4\" : ['total_invoice_amount', 'vendor_id','invoice_reference_number']}\n",
    "\n",
    "\n",
    "# create a function to return DF with test field\n",
    "def testBuilder(df, test_col_list, test_name):\n",
    "\n",
    "    #Creating temp DF for analysis\n",
    "    temp_dup_df = df.loc[:,test_col_list].dropna()\n",
    "\n",
    "    # find duplicated entries and create unique level dataframe\n",
    "    temp_dup_df = temp_dup_df.loc[temp_dup_df.duplicated(subset=test_col_list, keep=False),test_col_list]\n",
    "    temp_dup_df.drop_duplicates(inplace = True)\n",
    "    temp_dup_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "    # creating test group for each test -- any value means that the test was hit -- number indicates group number\n",
    "    temp_dup_df['{}_grp'.format(test_name)] = pd.Series(np.arange(1,len(temp_dup_df)+1,1))\n",
    "\n",
    "    # merging with parent dataframe using each \"duplicated key\" formed using columns for each test\n",
    "    df = df.merge(temp_dup_df, on=test_col_list, how = 'left', validate=\"many_to_one\")    \n",
    "\n",
    "    # return original dataframe with additional column for test applied\n",
    "    return df\n",
    "\n",
    "# apply tests for each tests defined in \"test_dict\"\n",
    "for test_iter in np.arange(0,len(test_dict)):\n",
    "\n",
    "    # define test column list and test name for testbuilder function\n",
    "    col_list = test_dict[list(test_dict.keys())[test_iter]]\n",
    "    name = list(test_dict.keys())[test_iter]\n",
    "\n",
    "    # apply testBuilder function for each test on groupby dataframe object\n",
    "    payments_df_ath = payments_df_ath.groupby(['company_code', 'cluster_num']).apply(lambda x : testBuilder(x, col_list, name)).reset_index(drop=True)\n",
    "\n",
    "# post formatting\n",
    "payments_df_ath.columns = payments_df_ath.columns.str.replace(\"_grp\", \"\")\n",
    "\n",
    "# change groups to flags\n",
    "# payments_df_ath.loc[:,payments_df_ath.columns.str.contains(\"[T*]\")] = (payments_df_ath.loc[:,payments_df_ath.columns.str.contains(\"[T*]\")].notnull()).astype(int)\n",
    "\n",
    "# transaction level score\n",
    "# payments_df_ath['transaction_score'] = payments_df_ath.loc[:,payments_df_ath.columns.str.contains(\"[T*]\")].sum(axis=1)\n",
    "test_weights = [3,2,1,3]\n",
    "# payments_df_ath['transaction_score'] = payments_df_ath.loc[:,payments_df_ath.columns.str.contains(\"[T*]\")].multiply(test_weights, axis = 'columns').sum(axis='columns')\n",
    "\n",
    "# new test_score\n",
    "payments_df_ath['test_score'] = (payments_df_ath.loc[:,payments_df_ath.columns.str.contains(\"[T*]\")].notnull()).astype(int).multiply(test_weights, axis = 'columns').sum(axis='columns')\n",
    "\n",
    "# payments_df_ath_org = payments_df_ath.copy()\n",
    "\n",
    "payments_df_ath['sequential_flag'] = 0.0\n",
    "payments_df_ath['T0'] = 0.0\n",
    "print(\"Part(2/6) - Overall 100% match DF Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2278.379150390625,
      "end_time": 1587039620399.234
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Forming DataFrames for ATH and Test Based Clustering\n",
    "\n",
    "# payments_df_ath = payments_df_ath_org.copy()\n",
    "\n",
    "payments_df_ath['test_score'].unique()\n",
    "payments_df_ath.shape\n",
    "payments_df_ath.loc[payments_df_ath['test_score'] == 0].shape\n",
    "\n",
    "# removing 0 test score (transactions not hitting any test)\n",
    "payments_df_ath = payments_df_ath.loc[payments_df_ath['test_score'] != 0]\n",
    "\n",
    "payments_df_ath.isna().sum()\n",
    "\n",
    "# creating test based clsuters\n",
    "payments_df_ath['T1_cluster_num'] = payments_df_ath['company_code'] + \"_\" + payments_df_ath['T1'].astype(str)\n",
    "payments_df_ath['T2_cluster_num'] = payments_df_ath['company_code'] + \"_\" + payments_df_ath['T2'].astype(str)\n",
    "payments_df_ath['T3_cluster_num'] = payments_df_ath['company_code'] + \"_\" + payments_df_ath['T3'].astype(str)\n",
    "payments_df_ath['T4_cluster_num'] = payments_df_ath['company_code'] + \"_\" + payments_df_ath['T4'].astype(str)\n",
    "\n",
    "# creating test based and ATH dataframes\n",
    "payments_df_test = payments_df_ath.loc[payments_df_ath['test_score'] != 9]\n",
    "payments_df_ath = payments_df_ath.loc[payments_df_ath['test_score'] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 59.698974609375,
      "end_time": 1587039620471.02
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(payments_df_ath.shape)\n",
    "print(payments_df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 47.406005859375,
      "end_time": 1587030927943.212
     }
    },
    "hidden": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 757.2470703125,
      "end_time": 1587039621249.223
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## ATH Clustering \n",
    "\n",
    "# cluster aggregated score\n",
    "# payments_df_ath['cluster_score'] = payments_df_ath.groupby(['company_code', 'cluster_num'])['transaction_score'].transform('max')\n",
    "\n",
    "# sort by company codes and cluster numbers\n",
    "payments_df_ath.sort_values(['cluster_num'], inplace=True)\n",
    "payments_df_ath = payments_df_ath.sort_values(['test_score', 'total_invoice_amount', 'invoice_date'], ascending=False)\n",
    "payments_df_ath = payments_df_ath.loc[payments_df_ath['Overall'].notnull(),:]\n",
    "payments_df_ath['Overall'].nunique()\n",
    "payments_df_ath['cluster_num'] = payments_df_ath['Overall'].astype(str) + \"_ath\"\n",
    "# payments_df_ath = payments_df_ath.loc[:,['Id','company_code', 'vendor_id', 'invoice_reference_number', 'invoice_date','total_invoice_amount','payment_accounting_document_number','cluster_num']]\n",
    "payments_df_ath['match_percent'] = 100\n",
    "\n",
    "# adding test based match percent and flags to improve test based fuzzy match percent code chunk\n",
    "for name in ['T1', 'T2', 'T3', 'T4']:\n",
    "    payments_df_ath['{}_%'.format(name)] = 100\n",
    "    payments_df_ath['{}_flag'.format(name)]  = 1\n",
    "\n",
    "def cluster_name(df):\n",
    "    df['cluster_num_new'] = df['company_code'] + \"_\" + (pd.Categorical(df['cluster_num']).codes + 1).astype(str)\n",
    "    return df\n",
    "\n",
    "payments_df_ath = payments_df_ath.groupby('company_code').apply(lambda x : cluster_name(x))\n",
    "payments_df_ath.drop(columns= ['cluster_num'], inplace = True)\n",
    "payments_df_ath.rename(columns = {'cluster_num_new' : 'cluster_num'}, inplace = True)\n",
    "\n",
    "# drop to get unique payment accouting document number across clusters\n",
    "payments_df_ath.drop_duplicates(['cluster_num', 'payment_accounting_document_number'], inplace=True)\n",
    "\n",
    "payments_df_ath['sequential_flag'] = 0.0\n",
    "payments_df_ath['T0'] = 0.0\n",
    "\n",
    "\n",
    "print(payments_df_ath.shape)\n",
    "print(\"Part(3/6) - ATH Part Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 31495.087890625,
      "end_time": 1587039652761.82
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# initiating test based final DF\n",
    "payments_df_test_append = pd.DataFrame()\n",
    "\n",
    "# iterate over each test to create overall cluster based on test cluster\n",
    "for name in ['T1', 'T2', 'T3', 'T4']:\n",
    "    try:\n",
    "        payments_df_single_test = payments_df_test.loc[payments_df_test[name].notnull()]\n",
    "        # sort by company codes and cluster numbers\n",
    "        payments_df_single_test.sort_values(['cluster_num'], inplace=True)\n",
    "        payments_df_single_test = payments_df_single_test.sort_values(['test_score', 'total_invoice_amount', 'invoice_date'], ascending=False)\n",
    "\n",
    "        payments_df_single_test = payments_df_single_test.loc[payments_df_single_test[name].notnull(),:]\n",
    "\n",
    "        payments_df_single_test['cluster_num'] = payments_df_single_test[name].astype(str) + \"_sth\"\n",
    "        payments_df_single_test['match_percent'] = 0\n",
    "\n",
    "        # adding test based match percent and flags to improve test based fuzzy match percent code chunk\n",
    "        for temp_name in ['T1', 'T2', 'T3', 'T4']:\n",
    "            if(temp_name == name):\n",
    "                payments_df_single_test['{}_%'.format(temp_name)] = 100\n",
    "                payments_df_single_test['{}_flag'.format(temp_name)]  = 1\n",
    "            else:\n",
    "                payments_df_single_test['{}_%'.format(temp_name)] = 0\n",
    "                payments_df_single_test['{}_flag'.format(temp_name)]  = 0\n",
    "\n",
    "\n",
    "        def cluster_name(df):\n",
    "            df['cluster_num_new'] = df['company_code'] + \"_\" + (pd.Categorical(df['cluster_num']).codes + 1).astype(str) + '_' + name \n",
    "            return df\n",
    "\n",
    "        payments_df_single_test = payments_df_single_test.groupby('company_code').apply(lambda x : cluster_name(x))\n",
    "        payments_df_single_test.drop(columns= ['cluster_num'], inplace = True)\n",
    "        payments_df_single_test.rename(columns = {'cluster_num_new' : 'cluster_num'}, inplace = True)\n",
    "\n",
    "        # drop to get unique payment accouting document number across clusters\n",
    "        payments_df_single_test.drop_duplicates(['cluster_num', 'payment_accounting_document_number'], inplace=True)\n",
    "\n",
    "\n",
    "        # remove clusters with single \n",
    "        accounting_num_count_df = payments_df_single_test.groupby('cluster_num')['payment_accounting_document_number'].agg({'count' : len, 'unique_num' : pd.Series.nunique})\n",
    "        accounting_num_count_df.reset_index(inplace = True)\n",
    "\n",
    "        # list of clusters to remove\n",
    "        clusters_to_remove = accounting_num_count_df.loc[(accounting_num_count_df['count'] == 1) | (accounting_num_count_df['unique_num'] == 1), 'cluster_num'].unique()\n",
    "        payments_df_single_test = payments_df_single_test.loc[~payments_df_single_test['cluster_num'].isin(clusters_to_remove)]\n",
    "\n",
    "        # append in each iteration\n",
    "        payments_df_test_append = payments_df_test_append.append(payments_df_single_test)\n",
    "        print(\"{} ||  Transactions  =>  {}  ||  Clusters => {}\".format(name,len(payments_df_single_test), payments_df_single_test['cluster_num'].nunique()))\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(\"Part(4/6) - Appending Test based DF Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 245.01904296875,
      "end_time": 1587039653024.724
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(payments_df_test.shape)\n",
    "print(payments_df_test_append.shape)\n",
    "print(payments_df_test_append['cluster_num'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 514788.11181640625,
      "end_time": 1587037706826.413
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "payments_df_test_append.drop(columns=['T1_%', 'T1_flag', 'T2_%', 'T2_flag','T3_%', 'T3_flag', 'T4_%', 'T4_flag'], inplace=True)\n",
    "payments_df_test_append['sequential_flag'] = 0.0\n",
    "\n",
    "\n",
    "def fuzz_test_creator(df):\n",
    "    temp_df = df.copy()\n",
    "    for test_iter in np.arange(0,len(test_dict)):\n",
    "\n",
    "        col_list = test_dict[list(test_dict.keys())[test_iter]]\n",
    "        name = list(test_dict.keys())[test_iter]\n",
    "        temp_df['match_key'] = temp_df[col_list].apply(lambda x : \"-\".join(x), axis = 1)\n",
    "        temp_df.sort_values(by = ['match_percent'], ascending=False, inplace=True)\n",
    "        key = temp_df.head(1)['match_key'].values[0]\n",
    "\n",
    "        matching_list = process.extract(key, temp_df['match_key'].unique(), limit=temp_df['match_key'].nunique())\n",
    "        matching_temp_df = pd.DataFrame(matching_list)\n",
    "        matching_temp_df = matching_temp_df.iloc[:,:2]\n",
    "\n",
    "        # rename columns\n",
    "        matching_temp_df.columns = ['match_key', '{}_%'.format(name)]\n",
    "\n",
    "        # single point comparison will alwasy be an exact match\n",
    "        if((len(matching_temp_df) == 1)):\n",
    "            matching_temp_df.iloc[0,1] = 100\n",
    "\n",
    "        # relative match percent with the closest match\n",
    "        temp_100_key = matching_temp_df.loc[matching_temp_df['{}_%'.format(name)] == 100, 'match_key']\n",
    "\n",
    "\n",
    "        if(temp_df.loc[temp_df['match_key'].isin(temp_100_key)].shape[0] < 2):\n",
    "            try:\n",
    "                matching_temp_df.iloc[0,1] = matching_temp_df.iloc[1,1]\n",
    "            except:\n",
    "                matching_temp_df = matching_temp_df.copy()\n",
    "\n",
    "        # test based cluster\n",
    "        matching_temp_df['{}_flag'.format(name)] = 0\n",
    "        matching_temp_df.loc[matching_temp_df['{}_%'.format(name)] >= 90, '{}_flag'.format(name)]  = 1\n",
    "\n",
    "#         display(matching_temp_df)\n",
    "        temp_df = temp_df.merge(matching_temp_df, on='match_key', validate='m:1', how = 'inner')\n",
    "\n",
    "\n",
    "\n",
    "    # Sequential Flag -------------->>>>\n",
    "    # checking integer invoice reference number for which sequence test will work\n",
    "    try:\n",
    "        temp_df['invoice_reference_number'] = temp_df['invoice_reference_number'].astype(float)\n",
    "        temp_df.sort_values('invoice_reference_number', inplace=True)\n",
    "        temp_df.loc[temp_df['invoice_reference_number'].diff()==1, 'sequential_flag'] = 1\n",
    "    except:\n",
    "        temp_df['sequential_flag'] = 0\n",
    "\n",
    "\n",
    "    # Typo Flag ------------------>>>>\n",
    "    typo_locs = []\n",
    "    id_list = list(map(str,temp_df['invoice_reference_number']))\n",
    "    for x,y in itertools.combinations(id_list, 2):\n",
    "        if(x == y):\n",
    "            continue\n",
    "        elif(Counter(x)==Counter(y)):\n",
    "            typo_locs.extend([x,y])\n",
    "\n",
    "    typo_locs = list(set(itertools.chain(typo_locs)))\n",
    "    temp_typo_flag_df = pd.DataFrame({'invoice_reference_number':typo_locs})\n",
    "    temp_typo_flag_df['cluster_num'] = temp_df['cluster_num'].unique()[0]\n",
    "\n",
    "    try:\n",
    "        # creating temp DF for typo flag and merging with parent\n",
    "        if(len(temp_typo_flag_df) > 0):\n",
    "            temp_typo_flag_df['T0']=1.0\n",
    "            temp_typo_flag_df = temp_typo_flag_df.reset_index(drop=True)\n",
    "            # merge with groupby parent\n",
    "            temp_df = temp_df.merge(temp_typo_flag_df,how = 'left', on = ['invoice_reference_number', 'cluster_num'], validate='m:1')\n",
    "            temp_df['T0'] = temp_df['T0'].replace(np.nan,0)\n",
    "    except:\n",
    "        temp_df['T0'] = 0\n",
    "\n",
    "    # return dataframe \n",
    "    return pd.DataFrame(temp_df)\n",
    "\n",
    "\n",
    "# Apply function to groupby object in parallel \n",
    "def applyParallel(dfGrouped, func):\n",
    "    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)\n",
    "    return pd.concat(retLst)\n",
    "\n",
    "\n",
    "payments_df_test_final = applyParallel(payments_df_test_append.groupby('cluster_num'), fuzz_test_creator)\n",
    "\n",
    "\n",
    "try:\n",
    "    payments_df_test_final['T0'] = payments_df_test_final['T0'].replace(np.nan,0.0)\n",
    "except:\n",
    "    payments_df_test_final['T0'] = 0.0  \n",
    "\n",
    "payments_df_test_final['sequential_flag'] = payments_df_test_final['sequential_flag'].replace(np.nan, 0.0)\n",
    "\n",
    "# clean\n",
    "payments_df_test_final.reset_index(drop=True, inplace=True)\n",
    "payments_df_test_final.drop(columns=['Overall_flag', 'match_percent'], inplace=True)\n",
    "payments_df_test_final.rename(columns={'Overall_%':'match_percent'}, inplace=True)\n",
    "\n",
    "# aligning columns\n",
    "payments_df_test_final = payments_df_test_final.loc[:,list(payments_df_ath.columns)]\n",
    "\n",
    "print(\"Part(5/6) - Fuzzy Matching Done!\")\n",
    "\n",
    "print(\" #####||| payment fuzzy DF ===> {} |||#####\".format(payments_df_test.shape[0]))\n",
    "print(\" #####||| payment fuzzy DF after Appending ===> {} |||#####\".format(payments_df_test_append.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 306.817138671875,
      "end_time": 1587037707255.81
     }
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "########################## Merging and Final formatting  ###############################\n",
    "#######################################################################################\n",
    "\n",
    "# append with 100% match DF\n",
    "final_df = payments_df_ath.append(payments_df_test_final, ignore_index=True)\n",
    "\n",
    "# correcting nan cluster numbers\n",
    "for clus in ['T1_cluster_num', 'T2_cluster_num', 'T3_cluster_num','T4_cluster_num']:\n",
    "    final_df.loc[final_df[clus].str.contains('_nan'), clus] = '-'\n",
    "\n",
    "final_df = final_df[['Id','cluster_num','match_percent','T1_%','T1_flag','T2_%','T2_flag','T3_%','T3_flag','T4_%','T4_flag','test_score','T1_cluster_num','T2_cluster_num','T3_cluster_num','T4_cluster_num', 'T0', 'sequential_flag']]\n",
    "final_df['Id'] = final_df['Id'].astype(int)\n",
    "data['Id'] = data['Id'].astype(int)\n",
    "\n",
    "# merging with original dataframe\n",
    "final_df = pd.merge(final_df,payments_data,how='left',on='Id', validate = 'm:1')\n",
    "\n",
    "# removing junk clusters with same JE ID \n",
    "accounting_num_count_df = final_df.groupby('cluster_num')['payment_accounting_document_number'].agg({'count' : len, 'unique_num' : pd.Series.nunique})\n",
    "accounting_num_count_df.reset_index(inplace = True)\n",
    "clusters_to_remove = accounting_num_count_df.loc[(accounting_num_count_df['count'] == 1) | (accounting_num_count_df['unique_num'] == 1), 'cluster_num'].unique()\n",
    "final_df = final_df.loc[~final_df['cluster_num'].isin(clusters_to_remove)]\n",
    "\n",
    "\n",
    "\n",
    "# renaming cluster numbers\n",
    "def cluster_name(df):\n",
    "    df['cluster_num_new'] = df['company_code'] + \"_\" + (pd.Categorical(df['cluster_num']).codes + 1).astype(str)\n",
    "    return df\n",
    "\n",
    "final_df = final_df.groupby('company_code').apply(lambda x : cluster_name(x))\n",
    "final_df.drop(columns= ['cluster_num'], inplace = True)\n",
    "final_df.rename(columns = {'cluster_num_new' : 'cluster_num'}, inplace = True)\n",
    "\n",
    "\n",
    "final_df[['match_percent', 'T1_%', 'T1_flag', 'T2_%', 'T2_flag', 'T3_%',\n",
    "       'T3_flag', 'T4_%', 'T4_flag', 'test_score']] = final_df[['match_percent', 'T1_%', 'T1_flag', 'T2_%', 'T2_flag', 'T3_%',\n",
    "       'T3_flag', 'T4_%', 'T4_flag', 'test_score']].astype(float)\n",
    "\n",
    "final_df.drop(columns=['Id'], inplace=True)\n",
    "\n",
    "print(\"Part(6/6) - Final Formatting Done!\")\n",
    "\n",
    "for test_iter in ['T1', 'T2', 'T3', 'T4']:\n",
    "    final_df.loc[final_df['{}_cluster_num'.format(test_iter)].str.contains(\".0\", regex = False),'{}_cluster_num'.format(test_iter)] = final_df.loc[final_df['{}_cluster_num'.format(test_iter)].str.contains(\".0\", regex = False),'{}_cluster_num'.format(test_iter)].str[:-2]\n",
    "\n",
    "# Final summary\n",
    "print('###############################################################################')\n",
    "print('Number of Transaction ==> {}'.format(final_df.shape[0]))\n",
    "print('Number of Clusters ==> {}'.format(final_df.cluster_num.nunique()))\n",
    "\n",
    "# by zone\n",
    "print(final_df.groupby('zone')['cluster_num'].agg({'Transactions' : len,\n",
    "                                                   'Clusters' : 'nunique'}).reset_index())\n",
    "                                    \n",
    "print('###############################################################################')\n",
    "\n",
    "# # save to parquet\n",
    "# final_df.to_parquet(self.parquet_save_path + f\"\\\\DP_INV_PAY_{self.source_system}.parquet\", engine='pyarrow', compression='gzip')\n",
    "# print(f\"{self.source_system} Parqeut Saved!\")\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.319px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 124.44422200000001,
   "position": {
    "height": "40px",
    "left": "1094.67px",
    "right": "20px",
    "top": "31px",
    "width": "410.222px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
